{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "de61f271",
      "metadata": {
        "id": "de61f271"
      },
      "source": [
        "# Train Word2Vec word embeddings, explore and visualize them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "834e6f4b",
      "metadata": {
        "id": "834e6f4b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6ac0f153",
      "metadata": {
        "id": "6ac0f153",
        "outputId": "2bd11a7d-bccc-4f29-99e9-037c96b56de0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Ensure NLTK resources are available\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "50d62454",
      "metadata": {
        "id": "50d62454"
      },
      "outputs": [],
      "source": [
        "# Load your data\n",
        "# file_path = '/content/combined_text_visual_labels.csv'\n",
        "file_path = '/content/messages.txt'\n",
        "# data = pd.read_csv(file_path)\n",
        "data = pd.read_csv(file_path, sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "1ad9459d",
      "metadata": {
        "id": "1ad9459d"
      },
      "outputs": [],
      "source": [
        "# Preprocessing function for text\n",
        "# def preprocess_text(text):\n",
        "#     # Lowercase\n",
        "#     text = text.lower()\n",
        "#     # Remove non-alphanumeric characters (basic cleaning)\n",
        "#     text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "#     # Tokenize\n",
        "#     tokens = word_tokenize(text)\n",
        "#     return tokens\n",
        "\n",
        "# Preprocessing function for text\n",
        "def preprocess_text(text):\n",
        "    # Check if the text is a string before applying lower()\n",
        "    if isinstance(text, str):\n",
        "        # Remove links and similar non-words using regular expressions\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n",
        "        text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
        "\n",
        "        # Lowercase\n",
        "        text = text.lower()\n",
        "        # Remove non-alphanumeric characters (basic cleaning)\n",
        "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "        # Tokenize\n",
        "        tokens = word_tokenize(text)\n",
        "        return tokens\n",
        "    else:\n",
        "        # Handle non-string values (e.g., float) - you can choose to skip them,\n",
        "        # replace them with an empty string, or handle them in a way that makes\n",
        "        # sense for your data.\n",
        "        return []  # Returning an empty list for non-string values\n",
        "\n",
        "\n",
        "# Apply preprocessing to the 'utterance' column\n",
        "data['tokens'] = data['utterance'].apply(preprocess_text)\n",
        "# print(data.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "72db317b",
      "metadata": {
        "id": "72db317b"
      },
      "outputs": [],
      "source": [
        "# Train Word2Vec model\n",
        "tokenized_corpus = data['tokens'].tolist()\n",
        "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "ec5bed88",
      "metadata": {
        "id": "ec5bed88"
      },
      "outputs": [],
      "source": [
        "# Extract word vectors for visualization\n",
        "words = list(model.wv.index_to_key)\n",
        "word_vectors = model.wv[words]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the vector for a specific word\n",
        "word_vector = model.wv['rapture']\n",
        "\n",
        "# # Example:\n",
        "# word_vector = model.wv['hello']\n",
        "print(word_vector)  # Prints the vector representation of 'hello'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyQTEZhECqse",
        "outputId": "6df6b01d-e443-4c87-910d-daf53f10c6bd"
      },
      "id": "FyQTEZhECqse",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-1.6375694   0.87641317  0.47428113 -0.6601066   1.1313608  -0.28947112\n",
            " -0.16658887  2.0931907   2.5158696  -0.19141366 -0.1302067  -0.77523404\n",
            "  0.6766735   0.55199283  0.72020084 -1.097189    0.65282977  0.11378887\n",
            "  0.13688046 -1.5672868   0.868893   -0.10696364  2.2126522   1.155414\n",
            "  0.6142786  -1.315182    0.09236972  0.5233969   0.06617496  0.59227705\n",
            " -0.29106268 -0.52912974 -0.97642875  0.19655797 -0.00990307  0.41919494\n",
            "  0.28485382 -0.16640693  0.36843815 -0.05643533  0.176577    0.3282708\n",
            "  0.6719951  -0.83766675 -0.12759058 -0.17451616 -1.3079484  -0.02385449\n",
            " -0.30156586 -0.40379208  0.5687018  -0.5405348   0.8332079  -0.6553475\n",
            "  0.7039992  -0.6530111   0.34768307  0.914566    0.3236766  -1.4503043\n",
            " -0.47211426  0.05285298  0.6264814   0.48881993 -0.05239639 -1.6242442\n",
            "  0.14549874  0.36125132  0.6770163   1.1054379   0.3455163   1.0821544\n",
            " -0.03672126 -0.6040956  -0.19070745  0.2411879  -0.17256866  0.7501581\n",
            "  0.2256451  -2.0480902  -0.67912954  0.6607992   0.29748806 -0.47330981\n",
            " -1.062708    0.2570853  -0.7238356  -0.31885985 -0.41151923  0.29301205\n",
            "  1.0537196   0.30003336 -1.043085   -0.8656464   0.26589128  0.57625103\n",
            " -0.30058295 -1.2828234  -0.53901756  0.8568212 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "0cd84b53",
      "metadata": {
        "id": "0cd84b53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a93055a-a607-4744-a5d6-764cfd534899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rapture: 1.0\n",
            "bliss: 0.8620722889900208\n",
            "piti: 0.8212586045265198\n",
            "sukkha: 0.8126170039176941\n",
            "sukha: 0.80827796459198\n",
            "pleasure: 0.8054018020629883\n",
            "joy: 0.793758749961853\n",
            "contentment: 0.7523196935653687\n",
            "tranquility: 0.7446420192718506\n",
            "lightness: 0.741054892539978\n",
            "calmness: 0.7373017072677612\n",
            "pleasantness: 0.7068516612052917\n",
            "spaciousness: 0.6907825469970703\n",
            "wave: 0.6906237602233887\n",
            "blissful: 0.6746405363082886\n",
            "waves: 0.6698039770126343\n",
            "euphoria: 0.6593618392944336\n",
            "bursts: 0.6495271921157837\n",
            "heaviness: 0.6328808665275574\n",
            "elation: 0.6288173794746399\n"
          ]
        }
      ],
      "source": [
        "# # Calculate cosine similarity between two word vectors\n",
        "# similarity = model.wv.similarity('word1', 'word2')\n",
        "\n",
        "# # Find similarity between words:\n",
        "# similarity = model.wv.similarity('hello', 'hi')\n",
        "# print(similarity)  # Prints the similarity score between 'hello' and 'hi'\n",
        "\n",
        "# Find the most similar words\n",
        "similar_words = model.wv.most_similar(positive=[word_vector], topn=20)\n",
        "\n",
        "# Print the results:\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c30fcd7e",
      "metadata": {
        "id": "c30fcd7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "outputId": "b8401d4b-51ea-4378-ff56-e87bdd2becbc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-9c52212ddeaa>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Reduce dimensions using t-SNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtsne\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTSNE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreduced_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtsne\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params_vs_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m         \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, skip_num_points)\u001b[0m\n\u001b[1;32m   1042\u001b[0m         \u001b[0mdegrees_of_freedom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m         return self._tsne(\n\u001b[0m\u001b[1;32m   1045\u001b[0m             \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0mdegrees_of_freedom\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_tsne\u001b[0;34m(self, P, degrees_of_freedom, n_samples, X_embedded, neighbors, skip_num_points)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"momentum\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m             \u001b[0mopt_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"n_iter_without_progress\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter_without_progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mopt_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m         \u001b[0;31m# Save the final number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_gradient_descent\u001b[0;34m(objective, p0, it, max_iter, n_iter_check, n_iter_without_progress, momentum, learning_rate, min_gain, min_grad_norm, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"compute_error\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_convergence\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0minc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/manifold/_t_sne.py\u001b[0m in \u001b[0;36m_kl_divergence_bh\u001b[0;34m(params, P, degrees_of_freedom, n_samples, n_components, angle, skip_num_points, verbose, compute_error, num_threads)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_embedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     error = _barnes_hut_tsne.gradient(\n\u001b[0m\u001b[1;32m    285\u001b[0m         \u001b[0mval_P\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0mX_embedded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m_barnes_hut_tsne.pyx\u001b[0m in \u001b[0;36msklearn.manifold._barnes_hut_tsne.gradient\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m_quad_tree.pyx\u001b[0m in \u001b[0;36msklearn.neighbors._quad_tree._QuadTree.build_tree\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mmax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2808\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2809\u001b[0m     \"\"\"\n\u001b[0;32m-> 2810\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2811\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Reduce dimensions using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "reduced_vectors = tsne.fit_transform(word_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8df73466",
      "metadata": {
        "id": "8df73466"
      },
      "outputs": [],
      "source": [
        "# Get the list of English stop words from nltk\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4443bad7",
      "metadata": {
        "id": "4443bad7"
      },
      "outputs": [],
      "source": [
        "# Random shuffle the words in the data to be able to look at a different subset every time\n",
        "#random.shuffle(words)\n",
        "\n",
        "# filter stopwords before display\n",
        "words_subset = [w for w in words if not w in stop_words][:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "219a6674",
      "metadata": {
        "id": "219a6674"
      },
      "outputs": [],
      "source": [
        "# Plotting the embeddings\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, word in enumerate(words_subset):\n",
        "    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
        "    plt.text(reduced_vectors[i, 0] + 0.1, reduced_vectors[i, 1] + 0.1, word, fontsize=9)\n",
        "plt.title('t-SNE visualization of Word2Vec word embeddings')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "digreg_env3810",
      "language": "python",
      "name": "digireg_env3810"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
