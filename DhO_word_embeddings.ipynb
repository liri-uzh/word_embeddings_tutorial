{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Visualizing Word Embeddings with Word2Vec\n",
    "\n",
    "## Introduction\n",
    "This tutorial demonstrates how to train Word2Vec word embeddings on text data and visualize the results. Word2Vec is a powerful technique that converts words into numerical vectors in a way that captures semantic relationships between words. Words that are used in similar contexts end up having similar vector representations.\n",
    "\n",
    "The main steps we'll cover:\n",
    "1. Setting up the required libraries\n",
    "2. Loading and preprocessing text data\n",
    "3. Training a Word2Vec model\n",
    "4. Exploring word relationships\n",
    "5. Visualizing word embeddings using t-SNE\n",
    "\n",
    "## Setup: Required Libraries\n",
    "First, we'll import all necessary libraries for our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code imports:\n",
    "- `pandas`: For data manipulation\n",
    "- `gensim.models.Word2Vec`: The main Word2Vec implementation\n",
    "- `sklearn.manifold.TSNE`: For dimensionality reduction and visualization\n",
    "- `matplotlib.pyplot`: For creating visualizations\n",
    "- `nltk`: For text processing utilities\n",
    "- Other utilities for regular expressions and random sampling\n",
    "\n",
    "## Downloading NLTK Resources\n",
    "We need to download some NLTK resources for text processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure NLTK resources are available\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These downloads provide:\n",
    "- `punkt`: For sentence tokenization\n",
    "- `stopwords`: Common words to filter out\n",
    "- `punkt_tab`: Additional tokenization data\n",
    "\n",
    "## Loading the Data\n",
    "Now we load our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "file_path = '/content/messages.txt'\n",
    "data = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data should be in a tab-separated format with an 'utterance' column containing the text to analyze.\n",
    "\n",
    "## Text Preprocessing\n",
    "We define a function to clean and tokenize our text data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Remove links and similar non-words using regular expressions\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)  # Remove links\n",
    "        text = re.sub(r'\\@\\w+|\\#', '', text)  # Remove mentions and hashtags\n",
    "\n",
    "        # Lowercase\n",
    "        text = text.lower()\n",
    "        # Remove non-alphanumeric characters (basic cleaning)\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        # Tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        return tokens\n",
    "    else:\n",
    "        return []  # Returning an empty list for non-string values\n",
    "\n",
    "# Apply preprocessing to the 'utterance' column\n",
    "data['tokens'] = data['utterance'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This preprocessing function:\n",
    "1. Checks if input is a string\n",
    "2. Removes URLs and social media markers\n",
    "3. Converts text to lowercase\n",
    "4. Removes special characters\n",
    "5. Tokenizes the text into individual words\n",
    "\n",
    "## Training the Word2Vec Model\n",
    "Now we train our Word2Vec model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Word2Vec model\n",
    "tokenized_corpus = data['tokens'].tolist()\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters explained:\n",
    "- `vector_size=100`: Length of the word vectors\n",
    "- `window=5`: Maximum distance between current and predicted word within a sentence\n",
    "- `min_count=1`: Ignores words that appear less than this\n",
    "- `workers=4`: Number of CPU cores to use for training\n",
    "\n",
    "## Extracting Word Vectors\n",
    "Extract the trained word vectors for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract word vectors for visualization\n",
    "words = list(model.wv.index_to_key)\n",
    "word_vectors = model.wv[words]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Word Relationships\n",
    "We can analyze relationships between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vector for a specific word\n",
    "word_vector = model.wv['rapture']\n",
    "print(word_vector)  # Prints the vector representation of 'rapture'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most similar words\n",
    "similar_words = model.wv.most_similar(positive=[word_vector], topn=20)\n",
    "\n",
    "# Print the results:\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the 20 words most similar to our target word, along with their similarity scores.\n",
    "\n",
    "## Visualizing Word Embeddings\n",
    "Finally, we'll visualize the word embeddings using t-SNE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce dimensions using t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(word_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "- `n_components=2`: Reduce to 2 dimensions for visualization\n",
    "- `random_state=42`: For reproducible results\n",
    "\n",
    "## Filtering Stop Words\n",
    "Remove common words before visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of English stop words from nltk\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random shuffle the words in the data to be able to look at a different subset every time\n",
    "words_subset = [w for w in words if not w in stop_words][:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Visualization\n",
    "Finally, create a scatter plot of the word embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the embeddings\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, word in enumerate(words_subset):\n",
    "    plt.scatter(reduced_vectors[i, 0], reduced_vectors[i, 1])\n",
    "    plt.text(reduced_vectors[i, 0] + 0.1, reduced_vectors[i, 1] + 0.1, word, fontsize=9)\n",
    "plt.title('t-SNE visualization of Word2Vec word embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a 2D visualization where:\n",
    "- Each point represents a word\n",
    "- Similar words appear closer together\n",
    "- The plot shows semantic relationships between words in your corpus\n",
    "\n",
    "## Conclusion\n",
    "This tutorial showed how to:\n",
    "1. Preprocess text data for word embedding\n",
    "2. Train a Word2Vec model\n",
    "3. Explore word relationships\n",
    "4. Visualize the resulting word embeddings\n",
    "\n",
    "The visualization can reveal interesting semantic relationships in your text data, showing how words cluster together based on their usage patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
